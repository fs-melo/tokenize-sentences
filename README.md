# tokenize-sentences
Tokenization is breaking a text chunk in smaller parts. Whether it is breaking Paragraph in sentences, sentence into words or word in characters.


![tokenization](Files/tokenization.png)

Here was maked tokenization from a text file to  with Python and NLTK API to a text file contending one tokenized sentence per line
